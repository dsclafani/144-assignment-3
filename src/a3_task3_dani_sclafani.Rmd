---
title: "a3_task3_dani_sclafani"
author: "Danielle Sclafani"
date: "2/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(textdata)
library(pdftools)
library(ggwordcloud)
library(here)
```


```{r, cache= TRUE}

twilight <- pdf_text(here("1-stephenie-meyer-twilight.pdf"))

```


```{r}
twl_tidy <- data.frame(twilight) %>% 
  mutate(text_full = str_split(twilight, pattern = "\\n")) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))
```


```{r}
# cutting out the preface and the information. want to start at chapter 1

twl_df <- twl_tidy %>% 
  slice(-(1:79)) %>% 
  mutate(chapter = case_when(
    str_detect(text_full, pattern = "1. FIRST") ~ text_full,
    str_detect(text_full, pattern = "2. OPEN")  ~ text_full,
    str_detect(text_full, pattern = "3. PHENOM")  ~ text_full,
    str_detect(text_full, pattern = "4. INVITATIONS")  ~ text_full,
    str_detect(text_full, pattern = "5. BLOOD")   ~ text_full,
    str_detect(text_full, pattern = "6. SCARY")  ~ text_full,
    str_detect(text_full, pattern = "7. NIGHTMARE")   ~ text_full,
    str_detect(text_full, pattern = "8. PORT")   ~ text_full,
    str_detect(text_full, pattern = "9. THEORY")  ~ text_full,
    str_detect(text_full, pattern = "10. INTERROGATION")  ~ text_full,
    str_detect(text_full, pattern = "11. COMPLICATION")  ~ text_full,
    str_detect(text_full, pattern = "12. BALANCING")  ~ text_full,
    str_detect(text_full, pattern = "13. CONFESSIONS")  ~ text_full,
    str_detect(text_full, pattern = "14. MIND")   ~ text_full,
    str_detect(text_full, pattern = "15. THE")  ~ text_full,
    str_detect(text_full, pattern = "16. CARLISLE") ~ text_full,
    str_detect(text_full, pattern = "17. THE")  ~ text_full,
    str_detect(text_full, pattern = "18. THE")  ~ text_full,
    str_detect(text_full, pattern = "19. GOODBYES")  ~ text_full,
    str_detect(text_full, pattern = "20. IMPAT")  ~ text_full,
    str_detect(text_full, pattern = "21. PHONE")  ~ text_full,
    str_detect(text_full, pattern = "22. HIDE")  ~ text_full,
    str_detect(text_full, pattern = "23. THE")  ~ text_full,
    str_detect(text_full, pattern = "24. AN") 
               ~ text_full)
    
  ) %>% 
  fill(chapter) %>% 
  separate(col = chapter, into = c("no", "title"), sep = " ") %>% 
  separate(col = no, into = c("no", "delete", sep = ".")) %>% 
  select(text_full, no, title) %>% 
  mutate(chapter = as.numeric(no))



```


```{r}
#getting word counts by chapter
twl_tokens <- twl_df %>% 
  unnest_tokens(word, text_full) %>% 
  select(chapter, word)

#twlight wordcount
twl_count <- twl_tokens %>% 
  count(chapter, word)

#removing stop words
twl_no_stop <- twl_tokens %>% 
  anti_join(stop_words)


#counting without stop words
nostop_counts <- twl_no_stop %>% 
  count(chapter, word)

# finding top 5 words in each chapter

top_10 <- nostop_counts %>% 
  group_by(chapter) %>% 
  arrange(-n) %>% 
  slice(1:10)

top_5 <- nostop_counts %>% 
  group_by(chapter) %>% 
  arrange(-n) %>% 
  slice(1:5)
```



```{r}
# lets look at this data

ggplot(data = top_5, aes(x = word, y = n)) +
  geom_col(fill = "purple")+
  facet_wrap(~chapter, scales = "free")+
  coord_flip()
```

```{r}
# many of the top 5 words are names, so going to remove names by creating a vector of the main characters names in order to get more accurate sentiments, villians names are ommitted from this list because important

names_vector <- c("bella", "edward", "carlisle", "alice", "jasper", "jacob", "charlie", "mike", "jessica", "cullen", "emmett", "esme", "angela", "billy")

name_df <- as.data.frame(names_vector) %>% 
  rename(word = names_vector)

no_names <- twl_no_stop %>% 
  anti_join(name_df)

```

```{r}
#making a visualization without names

counts_no_name <- no_names %>% 
  count(chapter, word)

no_name_top5 <- counts_no_name %>% 
  group_by(chapter) %>% 
  arrange(-n) %>% 
  slice(1:5)

ggplot(data = no_name_top5, aes(x = word, y = n)) +
  geom_col(fill = "purple")+
  facet_wrap(~chapter, scales = "free")+
  coord_flip()

```

```{r}
#breaking the book into beginning, middle and end

be_mid_end  <- no_names %>% 
  mutate(part = case_when(chapter %in% c(1, 2, 3, 4, 5, 6, 7, 8) ~ "Beginning",
    chapter %in% c(9, 10, 11, 12, 13, 14, 15, 16) ~ "Middle",
    chapter %in% c(17, 18, 19, 20, 21, 22, 23, 24) ~ "End"
  ))

counts_part <- be_mid_end %>% 
  count(part, word)

part_top10 <- counts_part %>% 
  group_by(part) %>% 
  arrange(-n) %>% 
  slice(1:10)

# visualizing word counts by part of the book

part_top10$part_reorder = factor(part_top10$part, levels = c("Beginning", "Middle", "End")) # added this line into code to make sure that the graph showed up as beginning, middle, end - the default was beginning, end, middle

ggplot(data = part_top10, aes(x = word, y = n)) +geom_col(fill = "cyan4") +
  facet_wrap(~part_reorder)+
  coord_flip()+
  theme_minimal()
```


### Making Wordclouds 

```{r}
## WITH names
#making a wordcloud
ch1_top100 <- nostop_counts %>% 
  filter(chapter == 1) %>% 
  arrange(-1) %>% 
  slice(1:100)

ch1_cloud <- ggplot(data = ch1_top100, aes(label = word))+
  geom_text_wordcloud(aes(color = n, size = n), shape = "circle")+
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c("lightgrey", "pink", "red"))+
  theme_minimal()

ch24_top100 <- nostop_counts %>% 
  filter(chapter == 24) %>% 
  arrange(-1) %>% 
  slice(1:100)

ch24_cloud <- ggplot(data = ch24_top100, aes(label = word))+
  geom_text_wordcloud(aes(color = n, size = n), shape = "circle")+
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c("lightgrey", "pink", "red"))+
  theme_minimal()

ch24_cloud

```



```{r}
## WITHout names
#making a wordcloud
ch1_noname_top100 <- counts_no_name %>% 
  filter(chapter == 1) %>% 
  arrange(-1) %>% 
  slice(1:100)

ch1_cloud_no_name <- ggplot(data = ch1_noname_top100, aes(label = word))+
  geom_text_wordcloud(aes(color = n, size = n), shape = "circle")+
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c("lightgrey", "pink", "red"))+
  theme_minimal()

ch24_noname_top100 <- counts_no_name %>% 
  filter(chapter == 24) %>% 
  arrange(-1) %>% 
  slice(1:100)

ch24_cloud_no_name <- ggplot(data = ch24_noname_top100 , aes(label = word))+
  geom_text_wordcloud(aes(color = n, size = n), shape = "circle")+
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c("lightgrey", "pink", "red"))+
  theme_minimal()

ch24_cloud_no_name

```

## Sentiments
```{r}
twl_affin <- no_names %>% 
  inner_join(get_sentiments("afinn"))

afinn_counts <- twl_affin %>% 
  count(chapter, value)

#distribution of afinn score of each chapter
ggplot(data = afinn_counts, aes(x = value, y =n))+
  geom_col()+
  facet_wrap(~chapter)

# mean afinn score of each chapter
mean_afinn <- twl_affin %>% 
  group_by(chapter) %>% 
  summarize(mean_afinn = mean(value))

#visualization of mean afinn score for each chapter
ggplot(data = mean_afinn,
       aes(x = fct_rev(as.factor(chapter)),
           y = mean_afinn)) +
  geom_col()+
  coord_flip()
```

### using NRC Lexicon to get more details about chapter sentiments
```{r}
twl_nrc <- no_names %>% 
  inner_join(get_sentiments("nrc"))

twl_nrc_counts <- twl_nrc %>% 
  count(chapter, sentiment)

ggplot(data = twl_nrc_counts, aes(x = sentiment, y =n)) +geom_col() +
  facet_wrap(~chapter)+
  coord_flip()
```

